{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine All JSON files into one Dataframe\n",
    "data_path = Path(\"../data/generated_raw_data/parsed/\").resolve()\n",
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for file in data_path.ls():\n",
    "    with file.open(\"r\") as f:\n",
    "        content = json.load(f)\n",
    "        all_sentences.extend(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip space in keywords\n",
    "for sentence in all_sentences:\n",
    "    sentence[\"keywords\"] = [kw.strip().lower() for kw in sentence[\"keywords\"]]\n",
    "    sentence[\"keywords\"] = [kw for kw in sentence[\"keywords\"] if len(kw) > 0]\n",
    "all_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the \"text\" key with \"sentence\" \n",
    "for sentence in all_sentences:\n",
    "    if \"text\" in sentence.keys():\n",
    "        sentence[\"sentence\"] = sentence[\"text\"]\n",
    "        del sentence[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to remove duplicates\n",
    "df = pd.DataFrame(all_sentences)\n",
    "df = df.drop_duplicates(subset='sentence', keep=\"last\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_words_list = [\n",
    "    \"report stolen\",\n",
    "    \"dues\",\n",
    "    \"cancel\",\n",
    "    \"cancellation\",\n",
    "    \"block\",\n",
    "    \"unblock\",\n",
    "    \"activate\",\n",
    "    \"activation\",\n",
    "    \"transfer\",\n",
    "    \"replacement\",\n",
    "    \"replace\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out where keywords have logged intents and which need manual intervention\n",
    "data_list = df.to_dict(orient=\"records\")\n",
    "intent_data_list = []\n",
    "unknown_intent = []\n",
    "for entry in data_list:\n",
    "    intersect = set(entry[\"keywords\"]).intersection(set(intent_words_list))\n",
    "    if len(intersect) == 1:\n",
    "        intent = str(intersect.pop())\n",
    "        entry[\"intent\"] = intent\n",
    "        entry[\"keywords\"].remove(intent)\n",
    "        intent_data_list.append(entry)\n",
    "    else:\n",
    "        unknown_intent.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unknown_intent), len(intent_data_list), len(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the intents to the messages which did not have an intent from the list above. If there was any garbage in that extraction, I removed it. So there should be a difference in the count of exported and imported csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(unknown_intent).to_csv(\"../data/unknown_intent_for_manual_tagging.csv\", sep=\",\", index=False) #export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_intent = pd.read_csv(\"../data/TAGGED_unknown_intent_for_manual_tagging.csv\").to_dict(orient=\"records\") #import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_data_list.extend(added_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_df = pd.DataFrame(intent_data_list).drop_duplicates(subset='sentence', keep=\"last\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_df.to_csv(\"../data/mark_for_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list which we mark for checking by someone other than me. This has some intents which are tagged manually, some which are from GPT3. There is a bit of chaos, and overlap and unclean intent labels (e.g. `cancel` and `cancellation`) as we would expect from human input in production as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_json(\"../data/generated_preprocessed.json\", orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
